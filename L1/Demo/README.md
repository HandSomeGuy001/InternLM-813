# InternLM2-1.8béƒ¨ç½²æµç¨‹
### éƒ¨ç½²è¯¦è§£  
æœ¬æ¬¡ä½¿ç”¨HuggingFaceçš„AutoModelsæ¥å£åŠ è½½ä¹¦ç”Ÿæµ¦è¯­å¤§æ¨¡å‹ï¼Œå¹¶åœ¨å‘½ä»¤è¡Œè¿›è¡Œäº¤äº’
å…·ä½“è¿è¡Œæµç¨‹å¦‚å›¾æ‰€ç¤º
```mermaid
graph LR
    style model_folder fill:#f9f,stroke:#333,stroke-width:2px
    style AutoModelsForCasualLLM fill:#bbf,stroke:#333,stroke-width:2px
    style AutoTokenizer fill:#bbf,stroke:#333,stroke-width:2px
    style user_input fill:#ffcc00,stroke:#333,stroke-width:2px
    style prompt fill:#ffcc00,stroke:#333,stroke-width:2px
    style message fill:#cfc,stroke:#333,stroke-width:2px
    style model fill:#cfc,stroke:#333,stroke-width:2px
    style response fill:#cfc,stroke:#333,stroke-width:2px

    model_folder-->|"åˆå§‹åŒ–ï¼šåŠ è½½æ¨¡å‹"|model
    model_folder-->|"åˆå§‹åŒ–ï¼šåŠ è½½åˆ†è¯å™¨"|AutoTokenizer
    user_input-->|"è¾“å…¥ï¼šç”¨æˆ·è¾“å…¥"|message
    prompt-->|"è¾“å…¥ï¼šç³»ç»Ÿæç¤º"|message
    message-->|"è¾“å…¥ï¼šåˆ†è¯å¤„ç†"|AutoTokenizer
    AutoTokenizer-->|"è¾“å…¥ï¼šè¾“å…¥æ¨¡å‹"|model
    model-->|"è¾“å‡ºï¼šç”Ÿæˆå“åº”"|response
```
### ä»£ç è¯¦è§£ï¼š
***
åˆå§‹åŒ–ï¼Œå¹¶æŒ‡å®šprompt
```python
model_name_or_path = "/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, device_map='cuda:0')
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='cuda:0')
model = model.eval()
system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
"""

messages = [(system_prompt, '')]
```
***
è¿è¡Œéƒ¨åˆ†
```python
while True:
    input_text = input("\nUser  >>> ")
    input_text = input_text.replace(' ', '')
    if input_text == "exit":
        break

    length = 0
    for response, _ in model.stream_chat(tokenizer, input_text, messages):
        if response is not None:
            print(response[length:], flush=True, end="")
            length = len(response)
```
å¯¹è¯ç¨‹åºä¸ºä¸€ä¸ªæ˜¾å¼çš„æ­»å¾ªç¯ã€‚ç¨‹åºé€šè¿‡inputæ–¹æ³•è¯»å–ç”¨æˆ·çš„è¾“å…¥ï¼Œinputæ–¹æ³•çš„å‚æ•°ä¸ºè¾“å…¥æç¤ºè¯ï¼Œç¨‹åºè¿è¡Œæ—¶æ˜¾ç¤ºåœ¨å‘½ä»¤è¡Œä¸­ï¼Œinputæ–¹æ³•çš„è¿”å›å€¼ä¿å­˜åœ¨å˜é‡input_textä¸­ï¼Œå¦‚æœinput_textçš„å†…å®¹æ˜¯"exit"ï¼Œåˆ™é€€å‡ºè¯¥æ­»å¾ªç¯ã€‚  
ç¨‹åºå°†åˆ†è¯å™¨ã€ç”¨æˆ·è¾“å…¥ã€å’Œæ¶ˆæ¯è®°å½•ï¼ˆé»˜è®¤ä¸ºpromptï¼‰è¾“å…¥åˆ°æ¨¡å‹è°ƒç”¨æ–¹æ³•stream_chatä¸­è·å–**æµå¼å“åº”**ã€‚æµå¼å“åº”ä½¿å¾—æ¨¡å‹çš„è¾“å‡ºæ›´åŠ å¹³æ»‘ï¼Œè€Œä¸æ˜¯ ç­‰ä¸€æ®µæ—¶é—´ ç„¶åè¾“å‡ºä¸€å¤§å †ï¼›ä¸ºäº†é€‚åº”æµå¼å“åº”è¾“å‡ºï¼Œæ‰“å°responseé‡‡å–äº†ç›¸åº”çš„ç­–ç•¥ã€‚printfä¸­çš„flushå‚æ•°ç¡®ä¿è·å–responseåç«‹å³è¾“å‡ºï¼Œendè®¾ç½®ä¸ºç©ºï¼Œé¿å…æ¢è¡Œã€‚  


### æ“ä½œæµç¨‹
- åˆ›å»ºé•œåƒä¸ºCUDA-12.2çš„å¼€å‘æœº  
- é…ç½®ç¯å¢ƒ  
```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n demo python=3.10 -y
# æ¿€æ´»ç¯å¢ƒ
conda activate demo
# å®‰è£… torch
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia -y
# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers==4.34 einops==0.8.0 sentencepiece==0.1.99
```
ğŸ‘†éƒ½æ˜¯æ‰‹å†Œ  
ğŸ‘‡è¡¥ä¸¤ä¸ªåº“  
```bash
pip install accelerate
pip install protobuf
```
ååˆ›å»º[cli_demo.pyæ–‡ä»¶](./cli_demo.py)  
æ‰§è¡Œ   
```bash
python cli_demo.py
```

### ä¸€äº›å°é—®é¢˜
åœ¨æ ¹æ® [æ–‡æ¡£](https://github.com/InternLM/Tutorial/tree/camp3/docs/L1/Demo) è¿›è¡Œæ“ä½œæ—¶ï¼Œæœ‰å‡ ä¸ªåº“æ²¡è£…ï¼Œå®Œå…¨æŒ‰ç…§ä¸Šè¿°æ–‡æ¡£æµç¨‹è¿›è¡Œï¼Œè¿è¡Œcli_demo.pyæ—¶ï¼Œä¼šé‡åˆ°å¦‚ä¸‹é—®é¢˜ï¼š
```bash
ImportError: 
InternLM2Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
```
çœ‹èµ·æ¥è¿˜éœ€è¦å®‰è£…protobuf
æ‰§è¡Œ pip install protobuf
```bash
ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`
```
çœ‹æ¥è¿˜éœ€è¦å®‰è£…accelerate
æ‰§è¡Œ pip install accelerate å
```bash
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.24.2 which is incompatible.
Successfully installed accelerate-0.33.0 huggingface-hub-0.24.2 psutil-6.0.0
```
å®‰è£…æˆåŠŸåå‡ºç°è¿™ä¸ªæç¤ºï¼Œä¸è¿‡è¿™ä¸ªé”™è¯¯å¹¶ä¸å½±å“æ¨¡å‹çš„è¿è¡Œ
### æœ€ç»ˆæ•ˆæœ
æ¨ç†èµ„æºå ç”¨å¦‚ä¸‹å›¾æ‰€ç¤º  
![](../../attachments/L1_Demo_Resources.png)  
æœ€ç»ˆå®ç°æ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤º  
![](../../attachments/L1_Demo_Output_Log.png)